{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive Long Form Question Answering System for Annual Reports using BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the brand statement of TCS?\""
      ],
      "metadata": {
        "id": "7RuTm1U7WFno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXTRACTING TEXT FROM PDF"
      ],
      "metadata": {
        "id": "_exM5acL1wRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six\n"
      ],
      "metadata": {
        "id": "REea6NOpTXDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 pdf2text.py /content/tcs.pdf >> converted_text.txt"
      ],
      "metadata": {
        "id": "OusqvzyVUi5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the text."
      ],
      "metadata": {
        "id": "gpxBlEvWLfVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "f1_new = open('/content/converted_text.txt', 'r')\n",
        "f1_old = open('cleaned_text.txt', 'w')\n",
        "#f1_beside = open('out3.txt', 'w')\n",
        "lines = f1_new.readlines()\n",
        "#print(lines)\n",
        "\n",
        "for line in range(len(lines)):\n",
        "  if(len(lines[line])>10 and re.search('[a-zA-Z]', lines[line]) and '|' not in lines[line]) :\n",
        "\n",
        "      if (len(lines[line+1])>10):\n",
        "          f1_old.write(lines[line].strip('\\n'))\n",
        "      elif (len(lines[line-1])>10 and re.search('[a-zA-Z]', lines[line-1]) and '|' not in lines[line-1]) and (len(lines[line+1]) < 10 and re.search('[a-zA-Z]', lines[line+1])) :\n",
        "       \n",
        "            f1_old.write(lines[line].strip('\\n'))\n",
        "            f1_old.write(lines[line+1].strip('\\n')+'\\n')\n",
        "            \n",
        "      elif (len(lines[line-1])>10 and re.search('[a-zA-Z]', lines[line-1]) and '|' not in lines[line-1]):\n",
        "\n",
        "            f1_old.write(lines[line].strip('\\n')+'\\n')"
      ],
      "metadata": {
        "id": "zZIbb3d71zLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using nltk to tokenize or separate the text into sentences."
      ],
      "metadata": {
        "id": "nvodDxOvLmjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  \n",
        "nltk.download('punkt')\n",
        "f_1 = open('/content/cleaned_text.txt', 'r')\n",
        "f_out = open('sentence_separated.txt', 'w')\n",
        "all_1 = f_1.readlines()\n",
        "all_sentences_text = []\n",
        "for i in all_1:\n",
        "  sentences = nltk.sent_tokenize(i)\n",
        "  for j in sentences:\n",
        "    j = j.replace(',', '')\n",
        "    j = j.replace('-', ' ')\n",
        "    # f_out.write(j+'\\n')\n",
        "    all_sentences_text.append(j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoqRVoGC6Ygl",
        "outputId": "d44e8fd6-0bff-4558-b27c-b93583899774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "L9Szaa6C8ACA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the TF-IDF Vectorizer and cosine similarity to compare all the sentences to the question and obtaining the top 200 sentences that are closest to the question. "
      ],
      "metadata": {
        "id": "p52eoIR8L6pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "all_sentences_text.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', question))\n",
        "tfidf = TfidfVectorizer().fit_transform(all_sentences_text)\n",
        "pairwise_similarity = tfidf * tfidf.T\n",
        "arr =  pairwise_similarity.toarray() \n",
        "temp = list(arr[len(arr)-1]).copy()\n",
        "temp.sort(reverse= True)\n",
        "to_finbert = []\n",
        "for i in range(200):\n",
        "  to_finbert.append(all_sentences_text[list(arr[len(arr)-1]).index(temp[i])])"
      ],
      "metadata": {
        "id": "EIN4vz6s8B6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINBERT"
      ],
      "metadata": {
        "id": "3my9ub_ESza3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain the finbert embeddings of the sentences and the question and once again use cosine similarity to rank the sentences by their distance from the question. Using finbert embedding lends financial context and just overall more contextual understanding than TF-IDF vectorizer."
      ],
      "metadata": {
        "id": "3uWEOG-VM-Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install finbert-embedding==0.1.4"
      ],
      "metadata": {
        "id": "0C3uc8_fZSB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wrapt --upgrade --ignore-installed"
      ],
      "metadata": {
        "id": "qdfnKOtOXVuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install finbert-embedding==0.1.4"
      ],
      "metadata": {
        "id": "INic9NP3XYTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from finbert_embedding.embedding import FinbertEmbedding\n",
        "finbert = FinbertEmbedding()"
      ],
      "metadata": {
        "id": "Z46mHUiDNmAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "X = question\n",
        "vectorX = finbert.sentence_vector(X)\n",
        "\n",
        "scores = list()\n",
        "\n",
        "for line in to_finbert:\n",
        "\n",
        "  Y = line\n",
        "  vectorY = finbert.sentence_vector(Y)\n",
        "  scores.append([Y, 1 - spatial.distance.cosine(vectorY, vectorX)])"
      ],
      "metadata": {
        "id": "XO4JpTo4Bbja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores.sort(key = lambda x: x[1], reverse=True)\n",
        "input_to_json_bert = open('input_to_json_bert.json','w' )\n",
        "top_k_sentences = []\n",
        "for i in range(20):\n",
        "  top_k_sentences.append(scores[i][0])"
      ],
      "metadata": {
        "id": "9RxhhSPoNDt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"The IRS Guidance pertaining to the subject. Best I can say is business expense deductible. But it depends on what it is you want to deduct. Taxpayer are considered “traveling away from home” if their duties. \""
      ],
      "metadata": {
        "id": "JeNTn0iAQ1W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(top_k_sentences)):\n",
        "  top_k_sentences[i] = x+top_k_sentences[i]"
      ],
      "metadata": {
        "id": "Wgh-_7u6Qef5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top k sentences after comparing finbert embeddings:"
      ],
      "metadata": {
        "id": "tGN7QRkbN2KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NghYDgdfkYTX",
        "outputId": "2d8154be-7172-409c-d583-abe5e0154083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is the brand statement of TCS ',\n",
              " 'WHAT DOES YOUR NEW BRAND STATEMENT `BUILDING ON BELIEF’ MEAN TO YOU AND TO CUSTOMERS?',\n",
              " 'TCS adopted a new brand statement Building on Belief to convey how its partnership with customers goes beyond technology deployment.',\n",
              " 'WHAT ARE TALENT CLOUDS AND HOW DO THEY CHANGE THE DELIVERY MODEL?',\n",
              " 'What is your company’s direct contribution to community development projects  Amount in INR and the details of the projects undertaken?',\n",
              " 'In that context the new brand statement is very timely and reflects that aspiration.',\n",
              " 'WHAT ROLE DOES YOUR INTELLECTUAL PROPERTY PRODUCTS AND PLATFORMS PLAY IN THE G&T OPPORTUNITY?',\n",
              " 'To better articulate its mission and its aspirations your company adopted a new brand statement this year `Building on Belief’.',\n",
              " 'If explanations provided here are found to be different from what is described in the Company’s periodic financial statements (not limited to Notes to Accounts) then the definition provided in the certified financial statements will prevail.',\n",
              " 'TCS Research marked its 40th year by adopting a new brand identity with the tagline `Inventing for Impact’.',\n",
              " 'I think it describes what TCS does very accurately and also reflects the ethos of the Tata Group and its evolution over the last century and a half.',\n",
              " 'Ernst & Young has assured the data presented under GRI Standards disclosures as specified in their Assurance Statement.',\n",
              " 'The new statement conveys how TCS builds on their ambition and optimism to transform their business for the better the impact of which is felt by their customers and the communities they serve.',\n",
              " 'The specific carbon footprint data is presented for the sake of continuity and is not comparable with the prior years.',\n",
              " 'TCS is committed to using zero ',\n",
              " 'This is why the new brand statement resonates so well and feels so right.',\n",
              " 'KAK: In the second scenario TCS Research & Innovation is a catalyst.',\n",
              " 'over and above what is mandated as per local laws?',\n",
              " 'For details of meetings of the Board please refer to the Corporate Governance Report which is a part of this report.',\n",
              " 'If yes what steps have been taken to improve their capacity and capability of local and small vendors?']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = dict()\n",
        "json_data[\"version\"] = \"v1\"\n",
        "json_paragraphs = list()\n",
        "\n",
        "for i in range(len(top_k_sentences)):\n",
        "  json_paragraph = {\"qas\":[{\"question\":question, \"id\":1, \n",
        "                            \"answers\":[{\"text\":\"An answer\", \"answer_start\":0}], \n",
        "                            \"is_impossible\": \"false\"}], \"context\":top_k_sentences[i]}\n",
        "  json_paragraphs.append(json_paragraph)\n",
        "\n",
        "json_data[\"data\"] = [{\"title\":\"TCS\", \"paragraphs\":json_paragraphs}]"
      ],
      "metadata": {
        "id": "lKTsbl1qPnfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('json_data.json', 'w') as f:\n",
        "  f.write(str(json_data))"
      ],
      "metadata": {
        "id": "vpf0yQcLPwvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "mYff0NDhSerP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section deals with downloading a pre-trained (on unlabelled annual report text) + finetuned (on FiQA) BERT model to rank your sentences to answer the question instead of similarity. In other words, the BERT model aids in ranking the sentence based on how well it answers the question. We then take the top 5 of those sentences and present them as an answer, thus completing the extractive long form question answering system using BERT.\n",
        "\n",
        "\n",
        "If you do not have your own pre-trained and fine-tuned BERT you can use the already fine-tuned question answering BERT models on hugging face. They yield very good results. This notebook has the continuing code: https://colab.research.google.com/drive/1anJQmmGilFkVg4UksLyZNOKyRH5Vf2b1?usp=sharing \n",
        "\n",
        "\n",
        "If you want to pre-train and fine-tune your model please visit this notebook. https://colab.research.google.com/drive/10QYn1F9AZUSiov9lqyxzjne41lvGHouV?usp=sharing "
      ],
      "metadata": {
        "id": "fgTH5RebOFdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cTIQpfi9Sgxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358abea1-b3b0-41a0-fc97-3f7972629f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "iUWWRLsWSkSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02797a9d-0a87-450b-cdef-733bae84d7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n"
      ],
      "metadata": {
        "id": "4TDGXU_LSm7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/uncased_L-12_H-768_A-12.zip"
      ],
      "metadata": {
        "id": "Gg9nzoSBSpOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0b3e7d-1b77-4f38-93df-fa945f25c6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ]
        }
      ]
    }
  ]
}